{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bac6a87",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia.You need to find following details: A) Rank B) Name C) Artist D) Upload date E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffd6da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: html5lib in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\hp\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca9d2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41016e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b46ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6637e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "rows = table.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85f175",
   "metadata": {},
   "source": [
    "# To handle the IndexError exception, We can wrap the code that causes the error  IndexError exception by checking the length of the cells list before accessing its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ee5a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cells list doesn't have at least 3 elements.\n"
     ]
    }
   ],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells) >= 3:\n",
    "        rank = cells[0].text.strip()\n",
    "        name = cells[1].text.strip()\n",
    "        artist = cells[2].text.strip()\n",
    "        views = cells[3].text.strip()\n",
    "        upload_date = cells[4].text.strip()\n",
    "    else:\n",
    "        print(\"The cells list doesn't have at least 3 elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d10eb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30. \"Humpty the train on a fruits ride\"[50] by Kiddiestv Hindi – Nursery Rhymes & Kids Songs | Upload date: January 26, 2018 | Views: 3.31\n"
     ]
    }
   ],
   "source": [
    " print(f\"{rank} {name} by {artist} | Upload date: {upload_date} | Views: {views}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7e93b",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv. \n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e926d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries, including requests and BeautifulSoup.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a688128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the website's URL and get the HTML content\n",
    "url = 'https://www.bcci.tv/'\n",
    "response = requests.get(url)\n",
    "html_content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9247be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121e1a8",
   "metadata": {},
   "source": [
    "# To fix this error, you can check if the find method returns a non-None value before calling the get method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d6d4bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the fixtures link\n"
     ]
    }
   ],
   "source": [
    "# Find the tag that contains the international fixtures information and extract its link\n",
    "fixtures_link_tag = soup.find('a', {'href': '/international/fixtures'})\n",
    "if fixtures_link_tag:\n",
    "    fixtures_link = 'https://www.bcci.tv' + fixtures_link_tag.get('href')\n",
    "else:\n",
    "    print('Could not find the fixtures link')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3a8ab",
   "metadata": {},
   "source": [
    "# To handle the NameError exception caused by the undefined fixtures_link variable, we can use a try-except block in our code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90a12822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'fixtures_link' variable is not defined.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(fixtures_link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "except NameError:\n",
    "    print(\"The 'fixtures_link' variable is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2627d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the tag that contains the Team India fixtures information\n",
    "team_india_fixtures_tag = soup.find('div', {'class': 'js-list'})\n",
    "if team_india_fixtures_tag:\n",
    "    # Extract the details of each fixture\n",
    "    fixtures = team_india_fixtures_tag.find_all('div', {'class': 'fixture__details'})\n",
    "    for fixture in fixtures:\n",
    "        match_title = fixture.find('span', {'class': 'u-unskewed-text'}).text.strip()\n",
    "        series = fixture.find('span', {'class': 'u-unskewed-text'}).find_next_sibling().text.strip()\n",
    "        place = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "        date = fixture.find('span', {'class': 'fixture__date'}).text.strip()\n",
    "        time = fixture.find('span', {'class': 'fixture__time'}).text.strip()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a9f178d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3403758745.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13088\\3403758745.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print('MatchTitle:', match_title)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Print the details of the fixture\n",
    "        print('MatchTitle:', match_title)\n",
    "        print('Series:', series)\n",
    "        print('Place:', place)\n",
    "        print('Date:', date)\n",
    "        print('Time:', time)\n",
    "        print()\n",
    "else:\n",
    "    print('Could not find Team India fixtures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1002b",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com. You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cab8beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "266246f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://statisticstimes.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97e50ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the website and get the HTML content\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46c314d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462425c1",
   "metadata": {},
   "source": [
    "# To handle this exception, wrap the find method call in a try-except block and catch the AttributeError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e5d1a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find link to 'States of India' page\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Find the link to the \"States of India\" page\n",
    "    states_link = soup.find('a', text='States of India').get('href')\n",
    "except AttributeError:\n",
    "    # Handle the exception\n",
    "    print(\"Could not find link to 'States of India' page\")\n",
    "    states_link = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1fd7d",
   "metadata": {},
   "source": [
    "# To handle this exception, check whether states_link is not None before concatenating it with the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f4c8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to 'States of India' page not found\n"
     ]
    }
   ],
   "source": [
    "if states_link is not None:\n",
    "    # Construct the URL for the \"States of India\" page\n",
    "    states_url = url + states_link\n",
    "else:\n",
    "    # Handle the case where the link was not found\n",
    "    states_url = url  \n",
    "    print(\"Link to 'States of India' page not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22676dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to 'States of India' page not found\n"
     ]
    }
   ],
   "source": [
    "# Initialize the states_url variable with a default value\n",
    "states_url = url\n",
    "\n",
    "if states_link is not None:\n",
    "    # Construct the URL for the \"States of India\" page\n",
    "    states_url += states_link\n",
    "else:\n",
    "    # Handle the case where the link was not found\n",
    "    print(\"Link to 'States of India' page not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c63099f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the \"States of India\" page and get the HTML content\n",
    "response = requests.get(states_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64ae3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8ce6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table containing the state-wise GDP data for the year 2019-20\n",
    "table = soup.find('table', {'id': 'table_id_20'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "78ac2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found\n"
     ]
    }
   ],
   "source": [
    "if table is not None:\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "    # Process the rows as needed\n",
    "else:\n",
    "    # Handle the case where the table was not found\n",
    "    rows = None  # or any other default value or action you want to take\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a88b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Baby Shark Dance\"[4] Pinkfong Baby Shark - Kids' Songs & Stories 12.37 June 17, 2016 [A]\n",
      "2. \"Despacito\"[7] Luis Fonsi 8.09 January 12, 2017 [B]\n",
      "3. \"Johny Johny Yes Papa\"[14] LooLoo Kids 6.63 October 8, 2016 [C]\n",
      "4. \"Bath Song\"[15] Cocomelon – Nursery Rhymes 6.03 May 2, 2018 [D]\n",
      "5. \"Shape of You\"[16] Ed Sheeran 5.92 January 30, 2017 [E]\n",
      "6. \"See You Again\"[18] Wiz Khalifa 5.79 April 6, 2015 [F]\n",
      "7. \"Phonics Song with Two Words\"[23] ChuChu TV 5.17 March 6, 2014 [G]\n",
      "8. \"Wheels on the Bus\"[24] Cocomelon – Nursery Rhymes 4.95 May 24, 2018 \n",
      "9. \"Uptown Funk\"[25] Mark Ronson 4.84 November 19, 2014 [H]\n",
      "10. \"Learning Colors – Colorful Eggs on a Farm\"[26] Miroshka TV 4.83 February 27, 2018 [I]\n",
      "11. \"Gangnam Style\"[27] Psy 4.70 July 15, 2012 [J]\n",
      "12. \"Masha and the Bear – Recipe for Disaster\"[32] Get Movies 4.53 January 31, 2012 [K]\n",
      "13. \"Dame Tu Cosita\"[33] El Chombo 4.24 April 5, 2018 \n",
      "14. \"Sugar\"[34] Maroon 5 3.82 January 14, 2015 \n",
      "15. \"Axel F\"[35] Crazy Frog 3.76 June 16, 2009 \n",
      "16. \"Roar\"[36] Katy Perry 3.73 September 5, 2013 \n",
      "17. \"Counting Stars\"[37] OneRepublic 3.73 May 31, 2013 \n",
      "18. \"Sorry\"[38] Justin Bieber 3.63 October 22, 2015 \n",
      "19. \"Thinking Out Loud\"[39] Ed Sheeran 3.55 October 7, 2014 \n",
      "20. \"Baa Baa Black Sheep\"[40] Cocomelon – Nursery Rhymes 3.51 June 25, 2018 \n",
      "21. \"Waka Waka (This Time for Africa)\"[41] Shakira 3.48 June 4, 2010 \n",
      "22. \"Dark Horse\"[42] Katy Perry 3.45 February 20, 2014 \n",
      "23. \"Faded\"[43] Alan Walker 3.41 December 3, 2015 \n",
      "24. \"Let Her Go\"[44] Passenger 3.38 July 25, 2012 \n",
      "25. \"Girls Like You\"[45] Maroon 5 3.37 May 31, 2018 \n",
      "26. \"Perfect\"[46] Ed Sheeran 3.37 November 9, 2017 \n",
      "27. \"Bailando\"[47] Enrique Iglesias 3.34 April 11, 2014 \n",
      "28. \"Lean On\"[48] Major Lazer 3.33 March 22, 2015 \n",
      "29. \"Lakdi Ki Kathi\"[49] Jingle Toons 3.32 June 14, 2018 \n",
      "30. \"Humpty the train on a fruits ride\"[50] Kiddiestv Hindi – Nursery Rhymes & Kids Songs 3.31 January 26, 2018 \n"
     ]
    }
   ],
   "source": [
    "# Loop through the rows and extract the required data\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) == 6:\n",
    "        rank = cells[0].text.strip()\n",
    "        state = cells[1].text.strip()\n",
    "        gsdp_18_19 = cells[2].text.strip()\n",
    "        gsdp_19_20 = cells[3].text.strip()\n",
    "        share_18_19 = cells[4].text.strip()\n",
    "        gdp = cells[5].text.strip()\n",
    "        \n",
    "        # Print the extracted data\n",
    "        print(rank, state, gsdp_18_19, gsdp_19_20, share_18_19, gdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65318d98",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com. \n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7e90935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f9333e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the Github trending page\n",
    "url = 'https://github.com/trending'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b054cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3a91d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the repository cards on the page\n",
    "repo_cards = soup.find_all('article', {'class': 'Box-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4c7147f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'card' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13088\\3186979915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrepo_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Find the repository description\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrepo_description\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'col-9'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Find the contributors count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'card' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through each repository card and extract the relevant details\n",
    "for card in repo_cards:\n",
    "    # Extract the repository title\n",
    "    repo_title = card.h1.a.text.strip()\n",
    "# Find the repository description\n",
    "repo_description = card.find('p', {'class': 'col-9'}).text.strip()\n",
    "\n",
    "# Find the contributors count\n",
    "contributors_count = card.find('a', {'class': 'muted-link d-inline-block mr-3'}).text.strip()\n",
    "\n",
    "# Print the repository name, description, and contributors count\n",
    "print(f\"Repository name: {repo_name}\")\n",
    "print(f\"Repository description: {repo_description}\")\n",
    "print(f\"Contributors count: {contributors_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d5d6efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2944716133.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7140\\2944716133.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    language = card.find('span', {'itemprop': 'programmingLanguage'})\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Extract the language used\n",
    "    language = card.find('span', {'itemprop': 'programmingLanguage'})\n",
    "    if language is not None:\n",
    "        language_used = language.text.strip()\n",
    "    else:\n",
    "        language_used = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6700b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (44774251.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7140\\44774251.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(f\"Title: {repo_title}\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  # Print the extracted details\n",
    "    print(f\"Title: {repo_title}\")\n",
    "    print(f\"Description: {repo_description}\")\n",
    "    print(f\"Contributors Count: {contributors_count}\")\n",
    "    print(f\"Language Used: {language_used}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73896d18",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. \n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1e9a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bd21ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.billboard.com/charts/hot-100\"\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81b45772",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "98248a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_details = []\n",
    "songs = soup.select('li > button > span.chart-element__information > span.chart-element__information__song')\n",
    "artists = soup.select('li > button > span.chart-element__information > span.chart-element__information__artist')\n",
    "last_week_ranks = soup.select('li > button > span.chart-element__meta.text--last')\n",
    "peak_ranks = soup.select('li > button > span.chart-element__meta.text--peak')\n",
    "weeks_on_board = soup.select('li > button > span.chart-element__meta.text--week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39b7f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if i < len(songs) and i < len(artists) and i < len(last_week_ranks) and i < len(peak_ranks):\n",
    "        song = songs[i].get_text()\n",
    "        artist = artists[i].get_text()\n",
    "        last_week_rank = last_week_ranks[i].get_text()\n",
    "        peak_rank = peak_ranks[i].get_text()\n",
    "        \n",
    "        print(f\"Song: {song}\")\n",
    "        print(f\"Artist: {artist}\")\n",
    "        print(f\"Last Week Rank: {last_week_rank}\")\n",
    "        print(f\"Peak Rank: {peak_rank}\")\n",
    "        print(\"-------------------------------\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "37be4950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(song_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6669f",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f96207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea9c3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49200c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table', {'class': 'in-article sortable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "470072cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = []\n",
    "for row in table.find_all('tr')[10:]:\n",
    "    cells = row.find_all('td')\n",
    "    book_name = cells[0].text.strip()\n",
    "    author_name = cells[1].text.strip()\n",
    "    volumes_sold = cells[2].text.strip()\n",
    "    publisher = cells[3].text.strip()\n",
    "    genre = cells[4].text.strip()\n",
    "    book_list.append((book_name, author_name, volumes_sold, publisher, genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1650b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book name: 9\n",
      "Author name: Angels and Demons\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 3,193,946\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 10\n",
      "Author name: Harry Potter and the Half-blood Prince:Children's Edition\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 2,950,264\n",
      "Genre: Bloomsbury\n",
      "\n",
      "\n",
      "Book name: 11\n",
      "Author name: Fifty Shades Darker\n",
      "Volumes sold: James, E. L.\n",
      "Publisher: 2,479,784\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 12\n",
      "Author name: Twilight\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,315,405\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 13\n",
      "Author name: Girl with the Dragon Tattoo,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 2,233,570\n",
      "Genre: Quercus\n",
      "\n",
      "\n",
      "Book name: 14\n",
      "Author name: Fifty Shades Freed\n",
      "Volumes sold: James, E. L.\n",
      "Publisher: 2,193,928\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 15\n",
      "Author name: Lost Symbol,The\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 2,183,031\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 16\n",
      "Author name: New Moon\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,152,737\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 17\n",
      "Author name: Deception Point\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 2,062,145\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 18\n",
      "Author name: Eclipse\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 2,052,876\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 19\n",
      "Author name: Lovely Bones,The\n",
      "Volumes sold: Sebold, Alice\n",
      "Publisher: 2,005,598\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 20\n",
      "Author name: Curious Incident of the Dog in the Night-time,The\n",
      "Volumes sold: Haddon, Mark\n",
      "Publisher: 1,979,552\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 21\n",
      "Author name: Digital Fortress\n",
      "Volumes sold: Brown, Dan\n",
      "Publisher: 1,928,900\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 22\n",
      "Author name: Short History of Nearly Everything,A\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 1,852,919\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 23\n",
      "Author name: Girl Who Played with Fire,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 1,814,784\n",
      "Genre: Quercus\n",
      "\n",
      "\n",
      "Book name: 24\n",
      "Author name: Breaking Dawn\n",
      "Volumes sold: Meyer, Stephenie\n",
      "Publisher: 1,787,118\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 25\n",
      "Author name: Very Hungry Caterpillar,The:The Very Hungry Caterpillar\n",
      "Volumes sold: Carle, Eric\n",
      "Publisher: 1,783,535\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 26\n",
      "Author name: Gruffalo,The\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 1,781,269\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 27\n",
      "Author name: Jamie's 30-Minute Meals\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 1,743,266\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 28\n",
      "Author name: Kite Runner,The\n",
      "Volumes sold: Hosseini, Khaled\n",
      "Publisher: 1,629,119\n",
      "Genre: Bloomsbury\n",
      "\n",
      "\n",
      "Book name: 29\n",
      "Author name: One Day\n",
      "Volumes sold: Nicholls, David\n",
      "Publisher: 1,616,068\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "\n",
      "Book name: 30\n",
      "Author name: Thousand Splendid Suns,A\n",
      "Volumes sold: Hosseini, Khaled\n",
      "Publisher: 1,583,992\n",
      "Genre: Bloomsbury\n",
      "\n",
      "\n",
      "Book name: 31\n",
      "Author name: Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\n",
      "Volumes sold: Larsson, Stieg\n",
      "Publisher: 1,555,135\n",
      "Genre: Quercus\n",
      "\n",
      "\n",
      "Book name: 32\n",
      "Author name: Time Traveler's Wife,The\n",
      "Volumes sold: Niffenegger, Audrey\n",
      "Publisher: 1,546,886\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 33\n",
      "Author name: Atonement\n",
      "Volumes sold: McEwan, Ian\n",
      "Publisher: 1,539,428\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 34\n",
      "Author name: Bridget Jones's Diary:A Novel\n",
      "Volumes sold: Fielding, Helen\n",
      "Publisher: 1,508,205\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 35\n",
      "Author name: World According to Clarkson,The\n",
      "Volumes sold: Clarkson, Jeremy\n",
      "Publisher: 1,489,403\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 36\n",
      "Author name: Captain Corelli's Mandolin\n",
      "Volumes sold: Bernieres, Louis de\n",
      "Publisher: 1,352,318\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 37\n",
      "Author name: Sound of Laughter,The\n",
      "Volumes sold: Kay, Peter\n",
      "Publisher: 1,310,207\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 38\n",
      "Author name: Life of Pi\n",
      "Volumes sold: Martel, Yann\n",
      "Publisher: 1,310,176\n",
      "Genre: Canongate\n",
      "\n",
      "\n",
      "Book name: 39\n",
      "Author name: Billy Connolly\n",
      "Volumes sold: Stephenson, Pamela\n",
      "Publisher: 1,231,957\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 40\n",
      "Author name: Child Called It,A\n",
      "Volumes sold: Pelzer, Dave\n",
      "Publisher: 1,217,712\n",
      "Genre: Orion\n",
      "\n",
      "\n",
      "Book name: 41\n",
      "Author name: Gruffalo's Child,The\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 1,208,711\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 42\n",
      "Author name: Angela's Ashes:A Memoir of a Childhood\n",
      "Volumes sold: McCourt, Frank\n",
      "Publisher: 1,204,058\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 43\n",
      "Author name: Birdsong\n",
      "Volumes sold: Faulks, Sebastian\n",
      "Publisher: 1,184,967\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 44\n",
      "Author name: Northern Lights:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,181,503\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "\n",
      "Book name: 45\n",
      "Author name: Labyrinth\n",
      "Volumes sold: Mosse, Kate\n",
      "Publisher: 1,181,093\n",
      "Genre: Orion\n",
      "\n",
      "\n",
      "Book name: 46\n",
      "Author name: Harry Potter and the Half-blood Prince\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 1,153,181\n",
      "Genre: Bloomsbury\n",
      "\n",
      "\n",
      "Book name: 47\n",
      "Author name: Help,The\n",
      "Volumes sold: Stockett, Kathryn\n",
      "Publisher: 1,132,336\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 48\n",
      "Author name: Man and Boy\n",
      "Volumes sold: Parsons, Tony\n",
      "Publisher: 1,130,802\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 49\n",
      "Author name: Memoirs of a Geisha\n",
      "Volumes sold: Golden, Arthur\n",
      "Publisher: 1,126,337\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 50\n",
      "Author name: No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\n",
      "Volumes sold: McCall Smith, Alexander\n",
      "Publisher: 1,115,549\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 51\n",
      "Author name: Island,The\n",
      "Volumes sold: Hislop, Victoria\n",
      "Publisher: 1,108,328\n",
      "Genre: Headline\n",
      "\n",
      "\n",
      "Book name: 52\n",
      "Author name: PS, I Love You\n",
      "Volumes sold: Ahern, Cecelia\n",
      "Publisher: 1,107,379\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 53\n",
      "Author name: You are What You Eat:The Plan That Will Change Your Life\n",
      "Volumes sold: McKeith, Gillian\n",
      "Publisher: 1,104,403\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 54\n",
      "Author name: Shadow of the Wind,The\n",
      "Volumes sold: Zafon, Carlos Ruiz\n",
      "Publisher: 1,092,349\n",
      "Genre: Orion\n",
      "\n",
      "\n",
      "Book name: 55\n",
      "Author name: Tales of Beedle the Bard,The\n",
      "Volumes sold: Rowling, J.K.\n",
      "Publisher: 1,090,847\n",
      "Genre: Bloomsbury\n",
      "\n",
      "\n",
      "Book name: 56\n",
      "Author name: Broker,The\n",
      "Volumes sold: Grisham, John\n",
      "Publisher: 1,087,262\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 57\n",
      "Author name: Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\n",
      "Volumes sold: Atkins, Robert C.\n",
      "Publisher: 1,054,196\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 58\n",
      "Author name: Subtle Knife,The:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,037,160\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "\n",
      "Book name: 59\n",
      "Author name: Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation\n",
      "Volumes sold: Truss, Lynne\n",
      "Publisher: 1,023,688\n",
      "Genre: Profile Books Group\n",
      "\n",
      "\n",
      "Book name: 60\n",
      "Author name: Delia's How to Cook:(Bk.1)\n",
      "Volumes sold: Smith, Delia\n",
      "Publisher: 1,015,956\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 61\n",
      "Author name: Chocolat\n",
      "Volumes sold: Harris, Joanne\n",
      "Publisher: 1,009,873\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 62\n",
      "Author name: Boy in the Striped Pyjamas,The\n",
      "Volumes sold: Boyne, John\n",
      "Publisher: 1,004,414\n",
      "Genre: Random House Childrens Books G\n",
      "\n",
      "\n",
      "Book name: 63\n",
      "Author name: My Sister's Keeper\n",
      "Volumes sold: Picoult, Jodi\n",
      "Publisher: 1,003,780\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "\n",
      "Book name: 64\n",
      "Author name: Amber Spyglass,The:His Dark Materials S.\n",
      "Volumes sold: Pullman, Philip\n",
      "Publisher: 1,002,314\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "\n",
      "Book name: 65\n",
      "Author name: To Kill a Mockingbird\n",
      "Volumes sold: Lee, Harper\n",
      "Publisher: 998,213\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 66\n",
      "Author name: Men are from Mars, Women are from Venus:A Practical Guide for Improvin\n",
      "Volumes sold: Gray, John\n",
      "Publisher: 992,846\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 67\n",
      "Author name: Dear Fatty\n",
      "Volumes sold: French, Dawn\n",
      "Publisher: 986,753\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 68\n",
      "Author name: Short History of Tractors in Ukrainian,A\n",
      "Volumes sold: Lewycka, Marina\n",
      "Publisher: 986,115\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 69\n",
      "Author name: Hannibal\n",
      "Volumes sold: Harris, Thomas\n",
      "Publisher: 970,509\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 70\n",
      "Author name: Lord of the Rings,The\n",
      "Volumes sold: Tolkien, J. R. R.\n",
      "Publisher: 967,466\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 71\n",
      "Author name: Stupid White Men:...and Other Sorry Excuses for the State of the Natio\n",
      "Volumes sold: Moore, Michael\n",
      "Publisher: 963,353\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 72\n",
      "Author name: Interpretation of Murder,The\n",
      "Volumes sold: Rubenfeld, Jed\n",
      "Publisher: 962,515\n",
      "Genre: Headline\n",
      "\n",
      "\n",
      "Book name: 73\n",
      "Author name: Sharon Osbourne Extreme:My Autobiography\n",
      "Volumes sold: Osbourne, Sharon\n",
      "Publisher: 959,496\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "\n",
      "Book name: 74\n",
      "Author name: Alchemist,The:A Fable About Following Your Dream\n",
      "Volumes sold: Coelho, Paulo\n",
      "Publisher: 956,114\n",
      "Genre: HarperCollins\n",
      "\n",
      "\n",
      "Book name: 75\n",
      "Author name: At My Mother's Knee ...:and Other Low Joints\n",
      "Volumes sold: O'Grady, Paul\n",
      "Publisher: 945,640\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 76\n",
      "Author name: Notes from a Small Island\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 931,312\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 77\n",
      "Author name: Return of the Naked Chef,The\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 925,425\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 78\n",
      "Author name: Bridget Jones: The Edge of Reason\n",
      "Volumes sold: Fielding, Helen\n",
      "Publisher: 924,695\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 79\n",
      "Author name: Jamie's Italy\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 906,968\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 80\n",
      "Author name: I Can Make You Thin\n",
      "Volumes sold: McKenna, Paul\n",
      "Publisher: 905,086\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 81\n",
      "Author name: Down Under\n",
      "Volumes sold: Bryson, Bill\n",
      "Publisher: 890,847\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 82\n",
      "Author name: Summons,The\n",
      "Volumes sold: Grisham, John\n",
      "Publisher: 869,671\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 83\n",
      "Author name: Small Island\n",
      "Volumes sold: Levy, Andrea\n",
      "Publisher: 869,659\n",
      "Genre: Headline\n",
      "\n",
      "\n",
      "Book name: 84\n",
      "Author name: Nigella Express\n",
      "Volumes sold: Lawson, Nigella\n",
      "Publisher: 862,602\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 85\n",
      "Author name: Brick Lane\n",
      "Volumes sold: Ali, Monica\n",
      "Publisher: 856,540\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 86\n",
      "Author name: Memory Keeper's Daughter,The\n",
      "Volumes sold: Edwards, Kim\n",
      "Publisher: 845,858\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 87\n",
      "Author name: Room on the Broom\n",
      "Volumes sold: Donaldson, Julia\n",
      "Publisher: 842,535\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 88\n",
      "Author name: About a Boy\n",
      "Volumes sold: Hornby, Nick\n",
      "Publisher: 828,215\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 89\n",
      "Author name: My Booky Wook\n",
      "Volumes sold: Brand, Russell\n",
      "Publisher: 820,563\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "\n",
      "Book name: 90\n",
      "Author name: God Delusion,The\n",
      "Volumes sold: Dawkins, Richard\n",
      "Publisher: 816,907\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 91\n",
      "Author name: \"Beano\" Annual,The\n",
      "Volumes sold: 0\n",
      "Publisher: 816,585\n",
      "Genre: D.C. Thomson\n",
      "\n",
      "\n",
      "Book name: 92\n",
      "Author name: White Teeth\n",
      "Volumes sold: Smith, Zadie\n",
      "Publisher: 815,586\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 93\n",
      "Author name: House at Riverton,The\n",
      "Volumes sold: Morton, Kate\n",
      "Publisher: 814,370\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "\n",
      "Book name: 94\n",
      "Author name: Book Thief,The\n",
      "Volumes sold: Zusak, Markus\n",
      "Publisher: 809,641\n",
      "Genre: Transworld\n",
      "\n",
      "\n",
      "Book name: 95\n",
      "Author name: Nights of Rain and Stars\n",
      "Volumes sold: Binchy, Maeve\n",
      "Publisher: 808,900\n",
      "Genre: Orion\n",
      "\n",
      "\n",
      "Book name: 96\n",
      "Author name: Ghost,The\n",
      "Volumes sold: Harris, Robert\n",
      "Publisher: 807,311\n",
      "Genre: Random House\n",
      "\n",
      "\n",
      "Book name: 97\n",
      "Author name: Happy Days with the Naked Chef\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 794,201\n",
      "Genre: Penguin\n",
      "\n",
      "\n",
      "Book name: 98\n",
      "Author name: Hunger Games,The:Hunger Games Trilogy\n",
      "Volumes sold: Collins, Suzanne\n",
      "Publisher: 792,187\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "\n",
      "Book name: 99\n",
      "Author name: Lost Boy,The:A Foster Child's Search for the Love of a Family\n",
      "Volumes sold: Pelzer, Dave\n",
      "Publisher: 791,507\n",
      "Genre: Orion\n",
      "\n",
      "\n",
      "Book name: 100\n",
      "Author name: Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\n",
      "Volumes sold: Oliver, Jamie\n",
      "Publisher: 791,095\n",
      "Genre: Penguin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the scraped details\n",
    "for book in book_list:\n",
    "    print(f\"Book name: {book[0]}\")\n",
    "    print(f\"Author name: {book[1]}\")\n",
    "    print(f\"Volumes sold: {book[2]}\")\n",
    "    print(f\"Publisher: {book[3]}\")\n",
    "    print(f\"Genre: {book[4]}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae23156",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278ad3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1ef6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/list/ls095964455/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f491bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c4d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81a99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list = soup.find_all('div', class_='lister-item-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4221d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = []\n",
    "for series in series_list:\n",
    "    name = series.h3.a.text.strip()\n",
    "    year_span = series.find('span', class_='lister-item-year').text.strip('()')\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    runtime = series.find('span', class_='runtime').text.strip()\n",
    "    try:\n",
    "        rating = float(series.find('div', class_='inline-block ratings-imdb-rating').strong.text)\n",
    "    except AttributeError:\n",
    "        rating = 0.0  # set default value\n",
    "    votes = int(series.find('span', attrs={'name': 'nv'})['data-value'])\n",
    "    series_data.append([name, year_span, genre, runtime, rating, votes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8705fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Year Span, Genre, Runtime, Ratings, Votes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(series_data, columns=['Name', 'Year Span', 'Genre', 'Runtime', 'Ratings', 'Votes'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa9e09",
   "metadata": {},
   "source": [
    "# 8. Details of Datasets from UCI machine learning repositories. \n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47951ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6cce2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make request to ShowAllDataset page\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9923da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cac0f8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13088\\3948862954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# find table with all datasets and extract the rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdatasets_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# find table with all datasets and extract the rows\n",
    "datasets_table = soup.find_all('table')[5]\n",
    "rows = datasets_table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ce909b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list to store dataset details\n",
    "datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b71d94f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13088\\1094371927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# loop through each row and extract dataset details\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# extract dataset name and link\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# loop through each row and extract dataset details\n",
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # extract dataset name and link\n",
    "    dataset_name = cells[0].find('a').text\n",
    "    dataset_link = cells[0].find('a')['href']\n",
    "    \n",
    "    # extract dataset details from the link's content\n",
    "    details_response = requests.get(dataset_link)\n",
    "    details_soup = BeautifulSoup(details_response.content, 'html.parser')\n",
    "    \n",
    "    # extract dataset attributes\n",
    "    dataset_attributes = details_soup.find_all('table')[0].find_all('tr')\n",
    "    data_type = dataset_attributes[0].find_all('td')[1].text.strip()\n",
    "    task = dataset_attributes[1].find_all('td')[1].text.strip()\n",
    "    attribute_type = dataset_attributes[2].find_all('td')[1].text.strip()\n",
    "    no_of_instances = dataset_attributes[3].find_all('td')[1].text.strip()\n",
    "    no_of_attributes = dataset_attributes[4].find_all('td')[1].text.strip()\n",
    "    year = dataset_attributes[5].find_all('td')[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7db20d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1782092514.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13088\\1782092514.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    dataset = {\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# add dataset details to the list\n",
    "    dataset = {\n",
    "        'name': dataset_name,\n",
    "        'data_type': data_type,\n",
    "        'task': task,\n",
    "        'attribute_type': attribute_type,\n",
    "        'no_of_instances': no_of_instances,\n",
    "        'no_of_attributes': no_of_attributes,\n",
    "        'year': year\n",
    "    }\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75b55638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print dataset details\n",
    "for dataset in datasets:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d8828",
   "metadata": {},
   "source": [
    "# 9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed291bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1b89ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "recruiters = soup.find_all('div', {'class': 'recInfo'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ea253c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recruiter in recruiters:\n",
    "    name = recruiter.find('span', {'class': 'fl ellipsis'}).text.strip()\n",
    "    designation = recruiter.find('span', {'class': 'ellipsis clr'}).text.strip()\n",
    "    company = recruiter.find('div', {'class': 'recInfo'}).find_all('a')[1].text.strip()\n",
    "    skills = recruiter.find('div', {'class': 'hireSec highlightable'}).text.strip()\n",
    "    location = recruiter.find('small', {'class': 'ellipsis'}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0e53bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1071067306.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13088\\1071067306.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print('Designation:', designation)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  print('Name:', name)\n",
    "    print('Designation:', designation)\n",
    "    print('Company:', company)\n",
    "    print('Skills:', skills)\n",
    "    print('Location:', location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
